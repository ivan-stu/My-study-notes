# **吴恩达 机器学习**

## 1、线性回归

### 1、基本术语
training set=训练集

x="input" variable feature

y="output" variable "target" variable

m=训练样本总数

(x,y)表示一个训练样本

(x^I,y^i)=1^th training example

### 2、监督学习的工作原理和流程
 一元线性回归：

![](静态资源/线性回归.png)

Linear regression:线性回归

Univariate linear regression:一元线性回归

为了训练好一个模型，有件事必须去做，那就是构造一个 **代价(成本)函数**

### 3、代价函数
1）代价函数公式：

(Cost function)代价函数是评价模型的一个指标，有助于我们去优化模型..用来衡量平方差的大小

![](静态资源/代价函数.png)

J(w,b):代价函数(方差代价函数)

2）代价函数的直观理解：

 choose w to minimize J(w)。(这是只有一个参数w的情况下)

 大多数时候不止w，还有b，，需要找到对应的w和b值去最小化J。

**所以，线性回归的目标就是找到参数w或w和b，使的代价函数的值最小**

3）可视化的代价函数

将三维碗形曲面图可视化为等高图

![](静态资源/碗形代价函数.png)

横坐标：w，纵坐标：b，函数值：J(w,b)

![](静态资源/可视化3D代价函数.png)

将三维碗形曲面图可视化为等高图. (最小椭圆代表最佳的(w,b)，使得J最小)

通过这种可视化，就能看到不同选择(w,b)是如何影响拟合数据的直线的.

### 4、梯度下降
1）不断找到局部下降最快的路径，不断重复该过程，直到到达局部最小值的地方

![](静态资源/梯度下降数学表示.png)

Simultaneously update:同时更新

    公式中的α：被称为学习率(learning rate)，学习率通常是个小正数，在0-1之间，α的作用它基本控制了你向下走的每一步步幅，如果α很大，导致一个非常激进的梯度下降过程，也就是你下降的步子会迈的很大，如果α很小，你就是一点点的往下走。 后面会学习如何选择一个好的学习率。

梯度下降是同时更新w，b值。

2）导数项的直观理解

![](静态资源/导数项的理解.png)

这里是讨论了w，让b=0

3）线性回归的梯度下降

![](静态资源/线性回归的梯度下降.png)

梯度下降原理：它可以导向局部最小值而不是全局最小值，全局最小值意味着在所有可能的J点中，它总是收敛于全局最小值。  

![](静态资源/碗形代价函数.png)

但是在线性回归中使用误差平方代价函数的时候，代价函数永远不会有多个局部最小值，只有一个全局最小值，因为代价函数是碗状的（凸函数）如上图。。所以对一个凸函数进行梯度下降的时候，有一个很好地特性，只要你学习率选择得当，它总是收敛于全局最小值。

4）批量梯度下降

![](静态资源/批量梯度下降.png)

这种梯度下降过程称为：
"Batch" gradient descent:**批量梯度下降**

批量梯度下降指每一步梯度下降，都会考虑到所有的训练样本而不是训练数据的子集。 

当然，也有其他梯度下降并不会看整个数据集，他们在每个更新步骤中查看小一点的训练数据子集。

## 2、多变量回归
### 1）多元特征

引入一些新符号：变量x1,x2,x3,x4来表示四个特征

X下标j，表示特征列表；

n表示特征的总数   例子中n=4；

X的上标，表示第i个训练样本；
例：x^(2)表示第二个训练样本的特征向量，也就是[1416,3,2,40]

x3^(2):表示第二个训练样本，第三个特征（j=3，下标）

下图：
![](静态资源/多特征.png)

    f w,b(x)=w1*x1+w2*x2+....+wn*xn+b

    w(->向量)=[w1 w2 w3 ....wn]

    x(->向量)=[x1 x2 x3 ... x4]

    dot product:点积

### 2）向量化

NumPy:一个库

Python数组下标从零开始

range(0,n)->表示0--(n-1)

![](静态资源/向量化.png)

np.dot(w,x)实现了向量w和x之间的点积。

NumPy的dot函数，是两个向量点乘运算的向量化实现。

**向量化的两个好处：1、减少了代码量；2、比不用向量化的算法运算更快（原因是dot函数能够调用计算机中并行的硬件，Cpu、Gpu）**

### 3）多元线性回归的梯度下降

![](静态资源/多元线性梯度下降.png)

其他算法：正规方程：求解w,b(了解)。

### 4）特征放缩(Feature scaling)

它能让梯度下降运行得更快。

总结：当你有不同的特征且取值范围差异较大，它可能会导致梯度下降运行缓慢，但通过重新放缩这些特征，使它们都具有可比较的值范围，可显著加快梯度下降速度。

具体怎么做的呢？？？？？ ↓↓↓

方法一：

例：300<=x1<=2000;
用x1除以max

即：0.15<=x1<=1


![](静态资源/法一.png)

法二：

均值归一化：重新缩放特征值，使得特征都以0为中心，通常在-1到1之间。

首先要找到平均值，也叫x1在训练集上的均值(希腊字母μ)
![](静态资源/均值归一化.png)

法三：

Z-score标准化（Z-score归一化/规范化）

（希腊字母σ：标准差）
![](静态资源/Z-score标准化.png).

以上。

在进行特征缩放时，最好把目标定为将每个特征的取值范围定在-1~1附近。。不要太大也不要太小。
![](静态资源/特征缩放范围.png)


### 5)检查梯度下降是否收敛

学习曲线：横轴代表梯度下降的迭代次数，纵轴代表成本J。

![](静态资源/梯度下降收敛.png)

上图中迭代次数到400次时，曲线已经变平，意味着梯度下降以及收敛，因为曲线不再下降了。

一般我们很难先知道迭代几次就收敛，所以可以先画个**学习曲线**，看看需要迭代多少次之后停止模型训练。

图右：使用自动收敛测试方法：

用ζ表示一个小数字的变量，如0.001，如果代价J迭代中减少的量小于这个数，可以看到曲线的最右边这部分是平坦的，可以说是收敛的。

记住：收敛就是指你找到了参数w和b接近J的最小可能值的情况。

选出正确的ζ是相当困难的，主要还是依赖于左边的图。

### **6)学习率的选择**

![](静态资源/选择学习率.png)

绘制代价-迭代次数图，注意到J有时上升有时下降，说明梯度下降工作不正常，可能意味着代码中有bug或学习率太大导致的。

上右图代价J一直变大，可能就是有bug。

正确实现梯度下降的一个调试技巧：在学习率足够小的情况下，每迭代一次，代价函数都应该减小。所以如果代价函数下降的不正常，可以将α设为一个很小很小的数字，看看是否每次迭代的代价都降低，如果即使α是一个很小的数，J不会每次迭代减小，反而增加，一般是代码有bug。

**尝试不同的一系列α值：**

![](静态资源/尝试α.png)

0.001  
0.01  
0.1  
1
少量迭代，绘制代价函数J；

试了0.001后,可以将各学习率提升三倍，每次选取是前一个值的三倍，做的就是尝试一系列α值，直到找的α值太小，且确保（代价下降太慢）；找到一个太大的α值(代价反复横跳);

然后我会慢慢选择最大的合适的学习率,(代价下降不慢、不横跳)或则比我找到的最大合理值稍微小一点，一般这么做，会选到一个合适的学习率。


### 7）特征工程

在许多实际应用中，选择或者输入合适的特征才是让算法正常工作的关键步骤。

**在特征工程中，运用你的知识或者直觉去设计新的特征，通常通过变换或合并问题的原始特征，使其帮助算法更简单的做出准确的预测，，这取决于你对实际应用的见解，而不是直接使用最开始有的特征，有时通过设计一个新特征，可能会得到一个更好的模型。**

例如：房子的长、宽，这是两个特征，设计出房子面积这一新的特征，能够更好匹配模型。

### 8）多项式回归

结合多元线性回归和特征工程的概念，提出多项式回归的新算法，将帮助我们将数据拟合成曲线、非线性函数。

例如：x，x^2,x^3

f=w1*x+w2*x^2+w3*x^3+b:选择这样的特征，那么特征的放缩会变得很重要，要放缩成可比较的值的范围。

### 9）

    Scikit-learn是一个非常广泛使用的开原机器学习库。


## 3、分类(Classification)

二分类问题：0 or 1

一对术语：正样本(true/one)和负样本(false/zero)
正负样本不意味着样本的好坏，只是说明是否、0/1的概念。

### 1)逻辑回归

逻辑回归的结果是拟合一条S型曲线去拟合数据

逻辑回归模型和公式：
![](静态资源/逻辑回归模型.png)

上图是：sigmoid函数图像，0<g(z)<1

z=w*x+b,有这个求出z值，再带入到sigmoid函数，得到输出值(预测值)g(z).

### 2)边界决策(Decision boundary)

z=w*x+b=0:决策边界

![](静态资源/决策边界.png)

根据sigmoid函数图像：
        当f(x)>=0.5? 是：y=1; 否：y=0 （y是预测值)

        即,当f(x)>=0.5--->g(z)>=0.5--->z>=0--->

        是：w*x+b>=0--->y=1

        否：w*x+b<0--->y=0

### 3)逻辑回归的代价函数

如何选择参数w和b？

事实证明，对于逻辑回归，平方误差模型函数并不是一个好的选择。

![](静态资源/逻辑回归：代价函数1.png)

相比于线性回归的代价函数，唯一改变就是把1/2放在了求和公式里面，而不是外面。

这里稍微改变一下代价函数J(w,b)的定义：把下边(loss)的式子，叫做**单个训练例子的损失**。大写L表示**损失函数**，它是关于f(x)和真实标签y的函数

下图就是逻辑回归 中用到的**损失函数**的定义(提出的关于逻辑回归损失函数的新定义)

![](静态资源/逻辑回归_代价函数2.png)

上图：(横坐标：预测值，纵坐标：损失值)

    y=1时：如果算法预测概论接近于1，而真实y是1，那么损失非常小，几乎是0，因为很接近正确答案了

![](静态资源/逻辑回归_代价函数3.png)

    y=0时：当f=0或非常接近0时，损失会非常小，这意味着预测的和真实值很接近。f值越大，损失越大，因为预测距离真实值越远。

### 4)逻辑回归的简化代价函数

注意：y取值：0或1

下图：简化的损失函数，当y取不同值时，就是原来的函数
![](静态资源/简化损失函数.png)

将简化的损失函数带入代价函数J(w,b),得到最终的代价函数。↓
![](静态资源/简化的代价函数.png)

### 5)逻辑回归的梯度下降

    重点就是如何找到最佳的参数w和b！！

    计算w、b的值，必须先算出图右的偏导数值，再代入到左边求w、b公式中，，注意，这些数据都是同步更新的。
![](静态资源/逻辑回归_梯度下降.png)

    逻辑回归的梯度下降与线性回归的梯度下降，不同的是对f(x)的定义↓↓
![](静态资源/逻辑回归_f定义比较.png)

### 6)过拟合、欠拟合

欠拟合：就是算法不能很好拟合训练数据，也就是模型对训练数据拟合不足----》算法有高偏差(high bias)

"泛化":算法能够适用于除训练集之外的样本。(意味着对没见过的样本也能做出好的预测)

过拟合：模型对训练集过于精准的拟合了，但是不具有泛化能力。(模型过拟合了数据)----》算法具有高方差(high variance)

    机器学习的目标就是找到一个既不欠拟合也不过拟合的模型！！

### 7)解决过拟合问题

方法一：收集更多的训练数据，是解决过拟合的首要工具。

方法二：观察是否可以使用更少的特征，(减少多项式)

方法三：正则化

    正则化做的是尽可能地让算法缩小参数的值，不一定要求把参数变成0
    正则化作用是，让你保留所有的特征，但防止特征权重过大，这有时会导致过拟合。

    按照惯例，只需要减小参数W的大小，参数b影响不大。

### 8)带有正则化的代价函数

正则化的思想就是，参数越小，模型可能会简单，也许是因为一个模型的特征变少了，那么过拟合的可能性也变小了

(普遍)实现正则化的方式是：惩罚所有的特征，准确说是惩罚所有的w参数

![](静态资源/正则化.png)
λ/2*m   ：lambda希腊字母。(它是正则化参数)
λ的理想选择是，能够平衡第一项和第二项：最小化均方误差和保持参数w较小

    上图公式：最小化第一项，可以让算法更好的拟合数据；最小化第二项，让参数w尽可能的小，减小过拟合的风险。

### 9)正则化线性回归实现梯度下降

![](静态资源/正则化_线性.png)



正则化逻辑回归：

在最小化逻辑回归的代价函数时，会惩罚参数w1，w2到wn，防止它们过大

![](静态资源/正则化_逻辑回归.png)

正则化逻辑回归实现梯度下降：

![](静态资源/正则化_逻辑_梯度下降.png)

    正则化逻辑回归梯度下降公式与正则化线性回归的梯度下降公式一样，就是对f函数的定义不同！！！！



# 神经网络(也称深度学习算法)

## 1、神经网络与直觉

### 1）神经元和大脑

    pass

### 2）需求预测

    输入层(输入向量X)、输出层、中间层叫做隐藏层。

    神经网络一个很好地特点是，当你从数据中训练它时，不需要明确的决定其他什么特征，神经网络可以自己计算出它想要在这个隐藏层中使用的特征。

## 2、神经网络模型

### 1）神经网络层(Neural network layer)

    层级，我们用上标[]来表示第几层的参数和层级。

    神经网络的工作原理：每一层输入一个数字向量，应用一堆逻辑回归单元，然后计算另一个向量，然后一层接着一层，直到你得到最终的输出层计算(就是神经网络的预测)，然后设置阈值或不做最终的预测。

![](静态资源/2_符号.png)

上图主要用来理解符号、上标、下标的意思：

    输入层通常是第0层，这里隐藏层有3层，第四层是输出层，所以这个神经网络一共有4层。

    看图上下面手写的式子:
    
        a表示激活值，上标L是指第L层的激活值,下标j表示第j个神经元，a[l-1]表示上一层的激活值。

        这里的g是sigmoid函数，在神经网络中g还称为激活函数。
        激活函数就是能够输出这些激活值的函数。

        输入向量x，它的另一个名字是a_0,这样方程也适用于第一层(l=0)

        这些就能够计算任意一层的激活值。

### 3）前向传播预测

    pass、(上课老师讲过)

## 3、TensorFlow框架

    TensorFlow是实现深度学习算法的主流框架之一。

    numpy是线性代数和Python的标准库。

![](静态资源/张量.png)
    张量(Tensor)就是一个多维数组,比如：一维数组：一维张量；；二维数组：二维张量(矩阵)....

### 1）数据在TensorFlow和Numpy中表示

    TensorFlow与Numpy表示矩阵有小小的差别：

    Numpy是如何存储向量和矩阵的：

        x=np.array([[2000.0,17.0]]):两个[],表示矩阵
        比如：x=np.array([[1,2,3],[4,5,6]]):表示2*3的矩阵

        比如：x=np.array([200,17]):一个[],表示一维向量
    
    TensorFlow表示矩阵：
        a1=layer_1(x),这里a1假设为[[0.2,0.7,0.3]]-->1*3的矩阵

        而TF表示为：
        tf.Tensor([[0.2 0.7 0.3]],shape=(1,3),dtype=float32)->shape()表示1*3的矩阵，dtype=float表示浮点型。

    如果你想将一个张量a1，转换回Numpy数组，可以使用函数a1.numpy(),它会获取相同的数据并以Numpy数组形式返回，而不会是TensorFlow数组或TensorFlow矩阵的形式。

### 2)构建一个神经网络

    之前：
        x=np.array([[200.0,17.0]])
        layer_1=Dense(units=3,activation="sigmoid")
        a1=layer_1(x)

        layer_2=Dense(units=1,activation="sigmoid")
        a2=layer_2(a1)
    
    每次计算一层。

    TensorFlow另一种方法：
        layer_1=Dense(units=3,activation="sigmoid")
        layer_2=Dense(units=1,activation="sigmoid")
        model=Sequential([layer_1,layer_2])

        x=np.array([[200.0,17.0],
                    [120.0,5.0],
                    [425.0,20.0],
                    [212.0,18.0]])
        y=np.array([1,0,0,1])
        model.compile(...)  #后面会讲
        model.fit(x,y)      #后面会讲

        model.predict(x_new)
    这里不用手动获取数据并传递给第一层，然后从第一层和第二层获取激活值。。Tf的顺序函数Sequential，能够将第一层和第二层串联起来形成一个神经网络。
    x输入数据、y是目标值。。units=3有3个单元的密集层。

    按照惯例。我们不指定层一、层二，而是直接放到顺序函数中，如下：

        model=Sequential([
            Dense(units=3,activation="sigmoid"),
            Dense(units=1,activation="sigmoid")])

## 4、实践神经网络

### 1）单层中的前向传播

    pass

## 5、AGI猜想

    AGI(Artificial General Intelligence):强人工智能

## 6、向量化计算

### 1）神经网络如何高效实现

    函数matmul()是实现矩阵乘法

    2）z向量=a向量和w向量的点积==a的转置*w

### 2）矩阵乘法代码

    Numpy数组：
    A=np.array([[1,-1,0.1],
                [2,-2,0.2]])

    AT=np.array([[1,2],
                [-1,-2],
                [0.1,0.2]])   ->AT表示A矩阵的转置
    AT=A.T：转置函数，用于矩阵转置

    W=np.array([3,5,7,9],
                [4,6,8,0])
    
    矩阵Z=np.matmul(AT,W)  有时会看到：Z=AT@W ->这也是调用matmul函数


### 3）前向传播的向量化实现

![](静态资源/向量矩阵乘法.png)

     AT=np.array([[200，,7]]) 

     W=np.array([[1,-3,5],
                [-2,4,-6]])
    
    b=np.array([[-1,1,2]])
    def dense(AT,W,b,g):
       z=np.matmul(AT,W)+b
       a_out=g(z)  #激活函数作用于z的每个元素上
       return a_out #>>>[[1,0,1]]

TensorFlow中有个约定，单样本实际上在矩阵X的行，而不是在矩阵X转置的行中。

## 6、训练神经网络

### 1）训练过程

![](静态资源/训练神经网络过程.png)

    第一步：指定模型
    第二步：要让TensorFlow编译模型，要指定想使用的损失函数，这次学习损失函数是（稀疏交叉熵）
    第三步：调用拟合函数，去拟合步骤一模型。epochs是专业术语，指想学习算法进行梯度下降的步数。（训练模型）

### 2）训练细节

1、第一步：
    指定给定输入x，参数w和b，以及如何计算输出代码片段模型

2、第二步：
    必须明确损失函数是什么，也将定义用来训练神经网络的代价函数

3、第三步：
    model.fit(X,y,epochs=)

## 7、激活函数

### 1）sigmoid函数的替代方案

![](静态资源/常用激活函数.png)

1、线性激活函数  
2、sigmoid激活函数  
3、ReLU激活函数，，这几个都是神经网络常用的激活函数

### 2）如何选择激活函数

    输出层：取决于目标的标签或标签y的真实值
        1、二元分类问题：建议使用sigmoid函数作为机会偶函数
        2、如果y是一个数，且可正可负，建议使用线性激活函数
        3、如果y值仅可以为正值或0或非负数，建议使用ReLU函数

    隐藏层：建议使用ReUL作为默认的激活函数，其他激活函数可以自学

### 3）激活函数的重要性

    常见的经验法则：不要在神经网络的隐藏层中使用线性激活函数！！

## 8、多分类

### 1）多类
     多分类是指：不只有可能输出标签的分类问题。

### 2）Softmax回归算法

    Softmax函数是逻辑回归的泛化。这是一种针对多分类环境的二元分类算法。

![](静态资源/Softmax.png)
    a1解释为：在给定输入特征x下，y=1的概率估计，其他同理。

    Softmax回归下的代价函数：

![](静态资源/Softmax损失函数.png)

Softmax的损失是loss(a1,...,an,y)

if y=i,,loss=-log(aj)::图像上图曲线

如果aj非常接近1，损失会非常小，aj越小，损失越大。这刺激了算法，让aj尽可能的大，尽可能接近1.

在输出层：推荐使用from_logits=True,让数值更精确

model.compile(loss=SparseCrossEntropy(from_logits=True))

### 3)多标签分类问题

    多分类：输出标签y可以是两类中的一类。或是两类以上中的一类。
    多标签分类：每张图片有很多的、与之相关的标签。比如识别一张图里是否有小车、人、大巴。  
    与单个输入有关，图像x是三个不同的标签。
    与多分类不同的是，比如，手写数字分类任务，输出y只是个数字，但这个数字有10种可能值。  上面识别图像的例子，他就是要解决三个二元分类的问题，输出y是是否有车、人、大巴。对应[0/1,0/1,0/1]。

## 9、优化

### 1)Adam优化算法

    Adam算法可以自动调整学习速率。Adam代表自适应矩阵估计(Adaptive Moment Estimation,A-D-A-M).
    Adam算法并不是全局都使用同一个学习率，模型的每个参数都会有不同的学习率。

    Adam算法就是，比如一个参数在梯度下降，大致在同方向运动，那么就会提高这个参数的学习率，向同方向运动的快一点；相反，如果一个参数来回振荡，算法就会把学习率变小一点，让它不会来回振荡。

    代码实现：
    就是给compile函数添加一个额外的阐参数：
        model.compile(optimizer=tf.Keras.optimizers.Adam(learning_rate=1e-3),
        loss=tf.Keras.losses.SparseCategoricalCrossentropy(from_logits=True))
    这个参数指定了你要用的优化器是：tf.Keras.optimizer.adam优化器，，Adam优化算法需要一些默认的初始学习率。

## 10、机器学习实践建议

### 1）模型评估

    70%训练集、30%测试集

为了评估模型，需要
测试集中的均方误差J_test、、、训练集上的表现指标J_train

![](静态资源/模型评估_线性.png)

![](静态资源/模型评估_分类.png)

比如：J_train训练集上表现很好(值小)、但测试集J_text很高，就会意识到：尽管它在训练集上表现很好，但实际推广到非数据集上的数据点是，模型的效果较差

    通过测试误差的表现来衡量算法的如何。

J_train、J_text还有更常用的定义：训练误差、测试误差

### 2）模型选择&交叉验证

该如何使用测试集，为给定的机器学习应用场景选择模型？

    我们把数据分为三个不同子集：训练集、交叉验证集(the cross-validation set)还有测试集。

这里用的还是10个训练的例子：10个不同的多项式

    60%数据放入训练集、20%给交叉验证集(cv)、20%给测试集

交叉验证集是一个额外的数据集，使用它来检查或信任检查不同模型的有效性或准确性。也称为(验证集)、(开发集)、(the dev set)

然后使用以下三个公式计算训练误差、交叉验证误差、测试误差。公式里都不包括正则化项
![](静态资源/误差公式.png)

![](静态资源/模型选择.png)

这些多项式去拟合参数w_1、b_1，但我们不用在测试集中求参数，我们在交叉验证集中计算这些参数，并计算J_cv(w1,b1)，类似的，第二个模型..第10个模型也是这样。。

为了选择模型，去观察那个模型的交叉验证集错误最低，比如，J_cv(w4,b4)最低，就选择这个四阶多项式作为这个应用模型，最后。如果想展示泛化误差估计这个模型在新数据上表现的怎样，就用测试集，求出J_text(w4,b4)。

    注意：：在整个过程，我们使用训练集拟合了这些参数，然后使用交叉验证集选择多多项式的次数，所以到目前，我们还没把任何参数w，b放到测试集中，，这就是为什么例子中J_text(w4,b4)将是这个模型泛化误差的公平估计.

这样模型选择流程更加完善，可以自动选择模型，，比如为线性回归模型，选择怎样的多项式，，当然也适用于其他模型的选择。

这样测试集会变得很公平(因为没有让模型提前偷看答案)，不会对泛化误差的过度乐观估计。

在最后确定模型后，才能用测试集去评估这个模型。

## 11、诊断方差偏差

在很多场景下，观察算法的偏差和方差能够很好的指导下一步该做什么。

大多数我们画不出F函数，无法直观的看到它是否表现的那么好，可以算法的偏差与方差↓

一个更系统的诊断或判断算法是否有高偏差或高方差的方法是：看你的算法在训练集和交叉集上的表现。

![](静态资源/方差偏差.png)

    左图：高偏差的特点：欠拟合
    J_train很高时，表明算法很高的偏差
    J_cv也很高

    右图：
    J_train 很低
    J_CV    很高
    表明有高方差特征：方差J_cv比J_train高得多。。就是说见过的数据要比没见过的表现得更好。

    中间图：
    J_train 很低
    J_CV    很低
    没有高偏差也没有高方差，适合的算法

![](静态资源/高方差、高偏差.png)

！！！

    高偏差意味着算法在训练集上表现不好，高方差意味着算法在检查验证集的表现比训练集上差很多

### 2）正则化&方差偏差

    下图：正则化参数的选择对偏差和方差，算法的整体性能产生的影响。
![](静态资源/正则化对方差偏差.png)

    正则化的参数λ表示你在两个目的之间的权衡程度：维持参数w小与训练集数据的拟合度。

正则化的参数λ取值极端：
![](静态资源/正则化取值.png)

    左图：参数λ很大，算法会卖力把w值压低，趋近于0，所以函数f=b，，就是常数。(欠拟合)

    右图：参数λ很小(相当于没有正则化)，算法会让w变大，，就会过拟合。

    中间：就是合适的参数λ,能够合适的拟合数据。

![](静态资源/选择正则化参数.png)
不断的尝试参数λ的值，最小化代价函数，得到参数w1，b1，再用交叉验证误差J_cv(w1,b1),尝试不同的参数λ，重复上述过程.....选出最合适的参数λ值。

    比如发现J_cv(w5,b5)交叉误差最低，就选择0.08作为参数λ值,,并选用w5，b5作为参数。

### 3）搭建性能基准

    为了判断训练误差是否高，观察训练差是否比人类表现水平高，更有用

    衡量人类的表现水平

建立基线水平常见方法就是衡量人类在这项任务上能做的多好。。

另一种估计性基线水平的方法是：其他人实现的算法的性能。

![](静态资源/基线水平.png)

Baseline performance与Training error差值可以判断是否有高偏差；

Training error与Cross validation error(J_cv) 差值可以判断是否有高方差

### 4）学习曲线

学习曲线是一种帮助你理解学习算法性能如何的方式，曲线随着经验的数量(训练样本数)发生变化。

学习曲线：横轴：训练样本数，，纵轴：误差(J_cv或J_train)

![](静态资源/学习曲线1.png)
总结：当只有少样本时，训练误差会偏小甚至为0，但训练集变大，二次函数更难拟合所有样本，误差就会变大。

交叉验证误差会随训练集变大，误差变小，，就是说学到到的模型在变好，交叉验证误差也会减小。

还有一个特点：交叉验证误差通常比训练误差高，因为会调整参数去拟合训练集，希望训练集上的性能提升。


看看高偏差的学习曲线：
![](静态资源/学习曲线2.png)

误差会趋于平稳状态(plateau)，因为当你获取到更多的样本，如果你拟合的是一个简单的线性函数，模型实际不会改变太多，误差就会趋于稳定。

  如果学习算法有很高的偏差，获取更多的训练集数据本身不会有很大的帮助。

高方差的学习曲线：

![](静态资源/学习曲线3.png)

加大训练集：
![](静态资源/学习曲线4.png)

    当方差很大时，扩大训练集会有很大帮助，训练集误差会变大，交叉验证误差会减小，这两个误差会接近人类平均水平。

绘制学习曲线有个缺点：使用不同大小的训练多种不同模型，需要昂贵的算力，所以实际中并不常见。

### 5）决定下一步

![](静态资源/方法.png)

    解决高方差：增加训练集、减少特征集、增加λ值(说明算法过拟合了数据，因为把太多注意力放在了训练集的拟合上，会牺牲泛化新例子上的能力，所以增加λ值会迫使算法去拟合更平滑的函数，也就是摆动幅度小的函数，来改善高方差的问题)

    解决高方差，主要两种方法：一是训练更多的数据、二是简化模型。。

    解决高偏差：增加特征集、增加多项式、减少λ值(正则参数减小，意味着我们要少关注后一项而多关注前一项，以期算法在训练集上做的更好)

    解决高偏差：主要使模型更强大或给予它更大的灵活性以拟合更复杂或更曲折的函数。

### 6）神经网络&方差与偏差

    神经网络提供了一种新方法解决方差与偏差。

![](静态资源/神经网络_方差.png)

    第一：拥有一个更大的神经网络不会有什么坏处，只要适当的正则化。
    一个警告：一个更大的新网络会减慢你的算法，但不会影响算法的性能，在大多数情况下，甚至会打打提升性能。

    第二：只要你的训练集不是太大，那么一种新网络，尤其是神经网络往往都是低偏差机器，它可以拟合非常复杂的函数。

## 12、机器学习发展历程

### 1）机器学习的迭代发展

![](静态资源/循环.png)

    首先，要决定系统的总体架构是什么，这意味着需要选择你的机器学习模型，以及决定使用什么数据。

    然后，给定这些决策，并实现并训练一个模型

    下一步，为了实现或者观察一些诊断，比如观察算法的偏差和方差，以及误差分析，，进行调整模型

    最后，带着新选择在进行一次循环，通常这个循环中进行多次迭代，直到到达想要的性能。

### 2）误差分析

具体来说，假设你有m_cv=500个交叉验证样本，错误分类了100个，，误差分析过程是指，人工检查这100这个样本，并试图找出算法出错的地方，，并试着按照共有的主题/属性/特征，把它们分组。

分析之后，会对下一步该做什么有启发，，比如是否要多收集一些数据、添加一些特征，等等，帮助提升算法。

### 3）添加数据

    数据增强

### 4）迁移数据



数据预处理

首先在大型数据集上进行训练，然后在较小的数据集上进一步参数调优，这两个步骤被称为监督预训练。


### 5）机器学习项目的全流程

机器学习项目
![](静态资源/项目流程.png)

    第一步：确定项目范围(我想做什么)
    第二步：收集数据
    第三步：初始数据收集完毕后，开始训练模型，进行误差分析改进模型
    最后 ：生产部署


## 13、数据倾斜

### 1）误差指标



### 2）权衡精确率&召回率

    手动选择

3-4）节，笔记以后再补充！！

## 14、决策树

### 1）测量纯度

    熵函数H():对一组数据不纯度的度量

![](静态资源/熵.png)

    p_1=样本中猫的比例
    当p_1=1/2时，p_1的熵是1，此时纯度最差，
    当p_1=0，p_1的熵是0，，等于0所以没有不纯度或者说这是一个完全纯的集合：所有都不是猫，所有都是狗。

熵函数的表达式：

![](静态资源/熵函数.png)

log是以2为底的，，0*log(0)实际上没有定义，是无穷小的，为了方便计算熵，让其等于0。

### 2）选择拆分信息增益

    熵的减小称为信息增益

我们不是计算加权平均熵，我们要计算熵的减小量和没有划分的情况相比。

计算出的数据：0.285，,0.03，，0.12被称为信息增益。它所衡量的是你的树在分裂过程中熵的减少。。

为什么我们要计算熵的减小而不是仅仅计算左、右分支的熵？

    事实证明：决定何时不再继续分裂的停止标准之一是，是否熵减少的足够少。。如果熵的减少量或者小于阈值，就不考虑分裂了。

0.28>0.12>0.03,,就会选择第一种。

信息增益的正式定义：

    w^left：左分支的样本比例；w^right：进入右分支的比例。

定义：

![](静态资源/信息增益.png)

    p_1^left：等于左子树中带有正标签例子的比例，正标签表示猫。。
    w^left：所有根节点到左边子分支的样本比例(5/10)。

    p_1^right：等于右子树中带有正标签例子的比例，正标签表示猫。。
    w^right：所有根节点到右边子分支的样本比例(5/10)。

    定义：p_1^root：为根节点为正的例子的比例(5/10)。

所以信息增益的定义为上图公式。

### 3）整合

以下是构建决策树的整个过程：

![](静态资源/决策树.png)

停止标准是：

    1、当一个节点100%是一个子句时，有的达到了零熵。
    2、或当进一步分裂一个节点将导致树的深度超过你设置的最大深度。
    3、或者如果信息增益来自一个另外小于阈值的分支。
    4、或者如果是例子的数值在节点中小于阈值

### 4）one-hot编码

猫狗分类，特征值只有两个，那特征取值是多个离散值呢？

    one-hot编码：
    比如：耳朵形状：再分为：是否是尖的、是否是耷拉的、是否是椭圆的。
    不是一个特征有三种可能值，而是创建了三个新特征，每个新特征都可以使用两个可能值中的一个：0/1

    通过one-hot编码，可以让你的决策树工作在可以取两个以上离散值的特征，你也可以应用这些特征到神经网络、线性回归或逻辑回归进行训练。

### 5）连续值的特征

比如猫狗体重，尝试某个不同体重阈值，做一般的信息增益计算和根据选定阈值划分特征，如果能给你所有可能的特征中最大的信息增益。

    总结一下：为了让决策树在每个节点上能处理连续值特征，当使用划分时，你只需要考虑不同值来划分，执行通常的信息增益计算，并决定对其进行分割，如果连续特征值能提供尽可能高的信息增益。

### 6）回归树

将决策树推广为回归算法。

方差：衡量一组数字的变化大小。

这里也是计算方差的减少量。

20.51是根节点上10个样本的方差。

![](静态资源/方差_决策树.png)

选择方差减少最大的那个。

## 15、集成学习

### 1）使用多个决策树

    树集合

训练一大堆不同的决策树，你会得到更准确的预测。

    使用树的集合的原因就是，拥有很多决策树，让它们投票，使你的整体算法对任何一棵树可能做的事情不那么敏感，因为一棵树只能投一票。。三棵树能投三票，你的整体算法变得更加健壮。

### 2）放回抽样

为了构建一个集成树，我们需要放回抽样技术。

采用抽样放回可以让你得到一个新的训练集，虽然和原始的有些像，但也有所不同。

### 3）随机森林算法

如何生成集成树模型：

    给定大小为m的训练集，对于，（for =1 to B：），使用有放回的方法创建大小为m的新的训练集，训练一个决策树，重复操作，可以总共做B次，B的典型选择是64-228之间的任何数值。。比如当生成100棵树时，想做预测，让这些树做最终的预测投票。

    事实证明，让B变大不会影响算法性能，但过了某个点后，收益会递减。

### 4）XGBoost

到目前为止，最常用的方法实现决策树集成的是叫做XGBoost的算法。

    XGBoost算法在第一次训练集上训练决策树，每次进入循环，不是选择每一次训练的例子，而是在概论相等的情况下，我们要选择概论更高的例子。

    但第二次通过这个循环，不是从所有m个等概率的例子中去取，我们更多可能选出之前训练过的树分类错误的例子或者之前训练过的树分类不好的例子，--》刻意训练

    这有助于学习算法更快的学习将性能做得更好。

### 5）什么时候使用决策树

决策树和集成树通常可以很好的处理表格数据(结构化数据)
意味着如果你的数据集是一个巨大的电子表格，值得考虑使用决策树。

相比之下，不建议在非结构化数据上使用决策树和集成树，比如图像、视频、音频和文本

决策树有个巨大的优势：计算快！

神经网络适用于所有类型的数据，包括结构化数据、非结构化数据以二者混合数据。

缺点：神经网络可能比决策树慢，一个大型的神经网络需要很长时间来训练。

神经网络其他好处包括可以和迁移学习一起用。

# 无监督学习/推荐系统/强化学习








